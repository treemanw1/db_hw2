
launch

AWS educate tutorial 1
- create instance
- record keypair pem file
- ssh into instance
	ssh -i ~/<path to pem file> ubuntu@<ip in console>
	ssh -i ~/.aws

SSH

hdfs dfs -mkdir -p /assignment2/part1/input

TRANSFER FILES BETWEEN SERVER AND LOCAL MACHINE

scp -i ~/home/treemanw1/learner_lab_keypair.pem ~/hw2/student_files/data/TA_restaurants_curated_cleaned.csv ec2-user@54.87.47.21:/home/ec2-user
scp -i learner_lab_keypair.pem ec2-user@34.230.17.158:/home/ec2-user/db_hw2/student_files/output/q1.csv/ ~/hw2/output/

hdfs dfs -put ~/TA_restaurants_curated_cleaned.csv /assignment2/part1/input/

SUBMITTING PY FILES

spark-submit --master spark://<internal-ip-of-spark-master>:7077 q1.py <internal-ip-of-spark-master>

spark-submit --master spark://172.31.29.165:7077 q1.py 172.31.29.165

GIT

sudo yum install git

COPY

	HDFS -> LOCAL
	hdfs dfs -get /path/to/hdfs/file /path/to/local/destination
	hdfs dfs -copyToLocal /assignment2/output/question1/part-00000-bd5b2e4b-fc2f-4e95-8f93-911e804d9059-c000.csv /output

To do:
- view assignment output
- connect to spark master ec2, clone my new repo
- edit and push to repo, git pull in ec2 ssh, test

13/4 Thursday
- view file in hdfs âœ“
scp -i learner_lab_keypair.pem ec2-user@34.230.17.158:/home/ec2-user/output/part-00000-bd5b2e4b-fc2f-4e95-8f93-911e804d9059-c000.csv hw2/output/

SUBMIT q1.py
spark-submit --master spark://172.31.29.165:7077 q1.py 172.31.29.165

COPY RESULT TO SERVER
hdfs dfs -copyToLocal /assignment2/output/question1/part-00000-4e979562-de40-416f-a9bd-063f2fe53f4f-c000.csv output

COPY RESULT TO LOCAL
scp -i learner_lab_keypair.pem ec2-user@34.230.17.158:/home/ec2-user/output/part-00000-bd5b2e4b-fc2f-4e95-8f93-911e804d9059-c000.csv hw2/output/

- get all of james answers
- save csvs locally
- test in 

- compare pandas result vs tio result
- learn to close cluster gracefully (flintrock)
- 

BEFORE YOU STOP ATTEMPT TO CLOSE GRACEFULLY

